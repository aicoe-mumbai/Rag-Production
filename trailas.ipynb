{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_saved_model/tokenizer_config.json',\n",
       " './my_saved_model/special_tokens_map.json',\n",
       " './my_saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", hf_token='#####')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    ").to(\"cuda\")\n",
    "save_directory = \"./my_saved_model\"  # Specify your desired directory\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "def generate_streaming_response(input_text):\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    thread = Thread(target=model.generate,\n",
    "                    kwargs={\n",
    "                        \"input_ids\": inputs['input_ids'],\n",
    "                        \"streamer\": streamer,\n",
    "                        \"max_new_tokens\": 256,\n",
    "                        \"temperature\": 0.4\n",
    "                    })\n",
    "    thread.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        if new_text.strip():\n",
    "            print(new_text, end=\"\")\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoe/Desktop/QA/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards:   0%|          | 0/8 [04:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/DeepSeek-R1-Distill-Qwen-32B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/DeepSeek-R1-Distill-Qwen-32B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./harshit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_dir)\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3974\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3971\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3973\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3974\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3985\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3990\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3991\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3992\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3993\u001b[0m ):\n\u001b[1;32m   3994\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1545\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1543\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1545\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1554\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1555\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    456\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/QA/llm-env/lib/python3.10/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "save_dir = \"./harshit\"\n",
    "model.save(save_dir)\n",
    "tokenizer.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_current_using_collection_value():\n",
    "#     try:\n",
    "#         current_collection = CurrentUsingCollection.objects.first()  \n",
    "#         if current_collection:\n",
    "#             # Assign the collection name to a variable\n",
    "#             collection_name = current_collection.current_using_collection\n",
    "#             return str(collection_name)\n",
    "#         else:\n",
    "#             return None \n",
    "#     except Exception as e:\n",
    "#         return str(e) \n",
    "\n",
    "collection_name = \"check\"\n",
    "\n",
    "if collection_name:\n",
    "    MILVUS_COLLECTION = collection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Use clear and concise language.\n",
      "\n",
      "\n",
      "### AI Assistant Response:\n",
      "Docker plays a crucial role in CI/CD pipelines, enabling consistent and reproducible deployments. Here are some best practices:\n",
      "\n",
      "* **Dockerfile:**  Use a Dockerfile to define your application's image. This file outlines the steps to build the image, including dependencies, libraries, and configurations.\n",
      "* **Multi-stage Builds:**  Employ multi-stage builds to optimize image size and reduce build times.\n",
      "* **Docker Hub:**  Store your Docker images on Docker Hub for easy sharing and access.\n",
      "* **Version Control:**  Use Git to manage your Docker images and ensure consistency across development and production environments.\n",
      "* **CI/CD Integration:**  Integrate Docker into your CI/CD pipeline using tools like Jenkins, GitLab CI/CD, or ArgoCD.\n",
      "* **Container Orchestration:**  Consider using Kubernetes for managing and scaling your Docker containers. "
     ]
    }
   ],
   "source": [
    "input_text = \"\"\" an AI assistant designed to assist users by providing simple and clear answers to their questions.\n",
    "\n",
    "### User Question:\n",
    "What are the best practices for using Docker in CI/CD pipelines?\n",
    "\n",
    "🔍 **Context:**  \n",
    "📘 The user has completed an **AWS with DevOps certification** from NareshIT Institute, covering topics such as:  \n",
    "- **AWS Services**: EC2, S3, Lambda, IAM, RDS, etc.  \n",
    "- **Infrastructure as Code (IaC)**: Terraform and Ansible  \n",
    "- **Containerization**: Docker and Kubernetes  \n",
    "- **CI/CD Tools**: Jenkins, Git, GitHub, GitLab, ArgoCD  \n",
    "- **Monitoring and Logging**: Grafana and Prometheus  \n",
    "- **Other Skills**: Linux, Git Bash, virtualization, orchestration  \n",
    "\n",
    "📝 The user is searching for a **cloud role** and is adaptable to any domain or role.\n",
    "\n",
    "**INSTRUCTIONS:**  \n",
    "- Your response should adhere closely to the context without adding anything else.\n",
    "- Make your response as precise as possible\n",
    "\"\"\"\n",
    "generate_streaming_response(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cohere_app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRAG_backend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcohere_app\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChunking_UI\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_document\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(process_document(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/QA/RAG_backend/cohere_app/Chunking_UI/file_process.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menable_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcohere_app\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChunking_UI\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m db_utility\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdoctr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentFile\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdoctr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ocr_predictor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cohere_app'"
     ]
    }
   ],
   "source": [
    "from RAG_backend.cohere_app.Chunking_UI.file_process import process_document\n",
    "\n",
    "print(process_document(\"/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(1, '     '), (2, ''), (3, '\\t'), (4, ''), (5, ''), (6, ''), (7, ''), (8, ''), (9, ''), (10, ''), (11, ''), (12, ''), (13, ''), (14, ''), (15, ''), (16, ''), (17, ''), (18, ''), (19, ''), (20, ''), (21, ''), (22, ''), (23, ''), (24, ''), (25, ''), (26, ''), (27, ''), (28, ''), (29, ''), (30, ''), (31, ''), (32, ''), (33, ''), (34, ''), (35, ''), (36, ''), (37, 'Table of Contents'), (38, ''), (39, '1.\\tPurpose\\t3'), (40, '2.\\tScope\\t3'), (41, '3.\\tExclusions (if any)\\t3'), (42, '4.\\tPrerequisites\\t3'), (43, '5.\\tProcedure\\t8'), (44, '6.\\tRoles & Responsibilities\\t8'), (45, '7.\\tReferences (Applicable Documents)\\t21'), (46, '8.\\tAbbreviations:\\t21'), (47, '9.\\tAdditional Points\\t21'), (48, '10.\\tRevision History\\t22'), (49, ''), (50, ''), (51, ''), (52, ''), (53, ''), (54, ''), (55, ''), (56, ''), (57, ''), (58, 'Purpose '), (59, 'The purpose of this Standard Operating Procedure is to establish a standardized and efficient process for identifying, documenting, and resolving non-conformities during execution of projects using the SARAL PLM system.'), (60, ''), (61, 'Scope'), (62, 'This SOP applies to all project stakeholders involved in the PLM system, including design, production, quality control, supply chain etc.'), (63, ''), (64, 'Exclusions (if any)'), (65, '--'), (66, ''), (67, 'Prerequisites'), (68, ' Definitions'), (69, 'Non-conformity'), (70, 'As per AS 9100D, a standard for quality management systems in the aerospace industry, nonconformity refers to the non-fulfillment of a specified requirement. It can relate to a product, process, or system, where the item does not meet the established standards or criteria. Nonconformity could be the result of an error, a deviation, or a failure to meet certain predefined requirements or specifications outlined by the standard. Identifying and addressing nonconformities is crucial for maintaining quality and ensuring compliance within the aerospace industry.'), (71, ''), (72, 'Requirement '), (73, 'Need or expectation that is stated, generally implied or obligatory '), (74, 'Note: '), (75, '“Generally implied” means that it is custom or common practice for the organization and interested parties that the need or expectation under consideration is implied. '), (76, 'A specified requirement is one that is stated, for example in documents'), (77, 'A qualifier can be used to denote a specific type of requirement, e.g. product requirement, quality management requirement, customer requirement, quality requirement.'), (78, 'Requirements can be generated by different interested parties or by the organization itself. '), (79, 'It can be necessary for achieving high customer satisfaction (3.9.2) to fulfil an expectation of a customer even if it is neither stated nor generally implied or obligatory. '), (80, ''), (81, 'Correction'), (82, 'Action taken to eliminate a detected nonconformity'), (83, 'Note:'), (84, 'A correction can be made in conjunction with a corrective action'), (85, 'For product non-conformity, correction might be understood as reworking the part, accepting the nonconformance through concession process, or scrapping the product'), (86, 'For a system issue, it may include correcting the paper work or issuing a new purchase order'), (87, 'For a delivery issue, it may include revising to air transportation instead of delivering product by truck or ship, increasing production rate, etc.'), (88, ''), (89, 'Containment action'), (90, 'Action to control and mitigate the impact of a problem and protect the organization and/or customer (i.e. stop the problem from getting worse), includes correction, immediate corrective action, immediate communication and verification that problem does not further degrade'), (91, ''), (92, 'Root cause'), (93, 'Actionable cause (it can be multiple)'), (94, ''), (95, 'Corrective action'), (96, 'Action to eliminate the cause of a nonconformity and to prevent recurrence'), (97, 'Note:'), (98, 'There can be more than one cause for a nonconformity. '), (99, 'Corrective action is taken to correct the recurrence.'), (100, 'The concept of preventive action has been replaced by Risk based thinking in the new versions of ISO 9001 and AS 9100. Also, there cannot be a preventive action after a NC has been raised.'), (101, ''), (102, 'RCA – Root Cause Analysis'), (103, 'It is a process of discovering the root cause of a problem and identify appropriate solutions for it.'), (104, 'Phases:'), (105, 'Naming the Problem'), (106, 'Understanding the parameters'), (107, 'Linking the problem to the parameters'), (108, 'Identifying a solution and testing its efficacy'), (109, ''), (110, 'Escalation Matrix'), (111, \"In the AS 9100D standard, which is specific to the aerospace industry, the escalation matrix for root cause analysis generally involves a step-by-step approach to resolving issues. Here's a typical escalation matrix that can be applied:\"), (112, 'Front-line employees: The first step is for front-line employees or operators to identify and report any issues they encounter during the manufacturing process.'), (113, 'Team leads or supervisors: If the issue cannot be resolved at the front-line level, it should be escalated to the respective team leads or supervisors who possess more authority and expertise in handling complex problems.'), (114, 'Quality assurance or quality control team: If the problem persists or requires further investigation, it should be escalated to the quality assurance or quality control team. '), (115, 'Management or cross-functional team: If the quality assurance team is unable to resolve the issue, it should be escalated to the management or a cross-functional team comprising individuals from various departments. This step involves a more comprehensive analysis and collaborative effort to resolve the issue.'), (116, 'Executive management: If the problem persists and cannot be resolved by the cross-functional team, it must be escalated to the executive management level. This includes high-level decision-makers who have the authority to allocate necessary resources and make crucial decisions to address the root cause effectively.'), (117, ''), (118, ' Non-conformity'), (119, 'Nonconformance or nonconformity is the failure to meet specified requirements, standards, or expectations. Non-conformance can occur on process, system and products or services. This may include instances of design deviation, production defects, material discrepancies, or any other departure from established standards'), (120, ''), (121, 'Non-conformity Report'), (122, 'A document that identifies and reports any discrepancies between the actual condition of a product or services/process/system and the defined requirements.'), (123, ''), (124, ' Types of Non-conformity'), (125, 'NC can be majorly categorized as below:'), (126, 'Major NC'), (127, 'A NC that directly affects the final output or an occurrence that could negatively impact function of product and its intended operations and objectives. It’s classified as a significant failure to meet the quality requirements'), (128, 'Multiple Minor NC’s observed in the same process/products or services/system can also be classified as Major NC'), (129, 'Also, the deviations from critical requirements defined in drawings'), (130, 'Repeated minor NC is also treated as a major NC'), (131, 'Minor NC'), (132, 'An occurrence where something isn’t followed according to requirements, but it doesn’t have the potential to impact the quality management system. '), (133, 'Minor NCR’s are seen as isolated issues that have limited risks to the organization'), (134, ''), (135, ''), (136, ''), (137, ''), (138, ''), (139, ''), (140, 'Identification of Non-conformity'), (141, ' NC’s can be identified through'), (142, 'Regular quality inspections'), (143, 'Testing'), (144, 'Customer feedbacks'), (145, 'Internal audits'), (146, 'Observations while performing an activity'), (147, ''), (148, ' Criteria for writing a Non-conformity'), (149, 'Before initiating a NCR, initiator shall have the following details:'), (150, 'The deviation details'), (151, 'An evidence for the non-conformance '), (152, 'Criticality of the NCR'), (153, 'The potential impact on project objectives '), (154, 'Risks related to NCR'), (155, ''), (156, ' Referring NC’s to Customer'), (157, ' Evaluation Criteria for Customer Referral'), (158, 'The criteria’s based on which customer is intimated about the NC’s are:'), (159, 'Contractual obligations (as defined in the contract / QAP)'), (160, 'Criticality of the non-conformity'), (161, 'Potential impact on the end product'), (162, 'Requirement of waiver'), (163, 'Testing required on the job after correction / corrective action'), (164, 'In case of a Customer complaint, details of the NC stage wise shall be shared'), (165, ''), (166, ' Communication Protocol with Customers'), (167, 'Customer shall be communicated through mail in the respective format defined in the contract / QAP.'), (168, 'Fax shall also be used to share the details of the NC to the customer.'), (169, ''), (170, 'Customer Notification and Follow-up'), (171, 'Stages where customer will be notified'), (172, 'If RCA is not applicable,'), (173, 'After implementation of possible corrections'), (174, 'As decision of waiver requirement is made'), (175, 'If RCA is applicable,'), (176, 'After implementation of correction'), (177, 'After Root cause analysis and definition of Corrective action to be taken'), (178, 'After final closure of NC'), (179, 'As and when the customer response is received, it will be documented.'), (180, ''), (181, ' Acceptance of Deviation by the Designer'), (182, ' Criteria for Accepting Deviation'), (183, 'Its impact does not affect the functionality of the job'), (184, 'Does not affect the job safety/ human safety'), (185, 'Does not affect the direct design requirements'), (186, ''), (187, ' Requesting Rework'), (188, 'Designer to request rework when a deviation cannot be accepted. Designer can request written procedure from responsible department along with RCA'), (189, ''), (190, ' Conducting Detailed Root Cause Analysis (RCA)'), (191, '  When to Perform RCA'), (192, 'The circumstances that warrant a detailed RCA are'), (193, 'Critical NC’s (Deviation in Key characteristics)'), (194, 'Critical to Quality deviations'), (195, 'Recurring/Repeated NC’s'), (196, 'Items that doesn’t yield the necessary performance'), (197, 'NC’s that affect the FFFI (Function Form Fit Integration)'), (198, 'Failure in following the QAP stage requirements'), (199, ''), (200, 'Guidelines for when to perform RCA'), (201, 'NCRs associated with critical product safety concerns or those that have the potential to impact customer safety or compliance with regulatory requirements. RCA is crucial for understanding the deep-seated causes of these issues and implementing effective corrective actions.'), (202, 'NCRs related to significant quality or performance deviations that could result in customer dissatisfaction or product recalls. Performing RCA helps us identify the underlying factors and implement preventive measures.'), (203, 'NCRs may not have an immediate, critical impact, they are still important as they represent recurrent issues or trends. By conducting RCA, we can proactively address systemic problems and improve overall quality.'), (204, ''), (205, ' RCA Methodologies'), (206, 'Why Why or 5 Why Analysis: A vertically structured way to handle the NC.'), (207, 'Statement of the problem is defined'), (208, 'The question “Why” the problem could have occurred is asked and defined'), (209, 'Each of the answer statement to the question “Why” becomes the new problem'), (210, 'The final Why concludes on the root cause of the problem'), (211, 'Action shall be taken on the root cause to eliminate the deviation and its recurrence'), (212, 'Note: Action can also be taken on all the other causes identified, to prevent deviation at any stages of a process.'), (213, 'Ishikawa or Fishbone Diagram or Cause and effect analysis'), (214, 'Statement of the problem is defined'), (215, 'Various categories from which problem can arrive are listed'), (216, 'Causes from each categories are branched'), (217, 'Action is taken on necessary causes to eliminate the NC'), (218, 'Pareto Chart: Series of bars in a graph reflects the frequency or impact of various problems'), (219, 'Statement of the problem is defined'), (220, 'Multiple causes are identified and listed'), (221, 'Causes are scored based on its impact'), (222, 'Bars are placed in the chart in descending order'), (223, 'Action is taken on the highest scored cause'), (224, ''), (225, 'Procedure'), (226, 'The following steps shall be followed through the process,'), (227, ''), (228, ' Writing a Non-conformity'), (229, ' Do’s & Don’ts for writing NC'), (230, 'Do’s'), (231, 'Write a complete statement covering all details'), (232, 'Keep targets for closure of each NCR stage'), (233, 'Avoiding speculation, and providing sufficient evidence.'), (234, 'Don’ts'), (235, 'Do not write assumptions/own personal feelings'), (236, 'Do not generalize or be vague'), (237, 'Do not miss out on any details pertaining to the NCR'), (238, ' NCR Cycle'), (239, \"One of the significant changes we've introduced is the option to select the NCR cycle based on the type of non-conformity. This means that each NCR will follow a cycle tailored to its specific requirements, streamlining the entire process. There are 4 NCR Cycles, which is decided by designer at design review stage by selecting RCA required Yes/No & Disposition by Designer Accept as it is, accept partial Quantity, Confirm FFFI, Correction/rework, Reject. Cycle is described as below: \"), (240, 'Design Confirmation'), (241, \"Minor deviation which doesn't affect functionality\"), (242, 'Validation by RCA'), (243, 'Deviation is not affecting the function or fitment but RCA is required to avoid re-occurrence'), (244, 'Confirm FFFI (Form, Fit, Function & Integration)'), (245, 'Deviation can be accepted based on satisfactory fitment of item or functional performance of product '), (246, 'Correction & RCA'), (247, 'Major deviation where Correction, Corrective Action (CA) is required'), (248, ''), (249, ' Steps in NCR Cycle'), (250, ''), (251, 'Create: (Person who first identifies the deviation)'), (252, 'NCR to be created on project for which deviation item belongs. In case of main project and sub-project is defined in PLM, NCR to be raised on sub-project only. This will help to analyses data by sub-project. '), (253, ''), (254, 'Description: Write a complete statement covering all details of NC'), (255, 'NCR location: where NC is observed shop, store, supplier or customer site'), (256, 'Work Center: useful to identify shop location\\t'), (257, 'Name of Responsible department manager, design engineer, QC engineer is defined by PMG. In case of any addition to be asked to PMG'), (258, 'Category: it is category of item which has NC. Mechanical, Electrical, FIM or FAT, site Trials for system level NC'), (259, ''), (260, 'Sub category: based on category selected system will show options for sub category '), (261, 'Category description: it is referred as “Type of deviation\"'), (262, 'Responsible department: The Responsible Department field in the Non-Conformity Report typically designates the department or team that is primarily responsible for addressing and resolving the reported non-conformity. This field helps in ensuring clear accountability and streamlining the process of rectifying the reported issues.'), (263, 'Stage of inspection: stage in which NC is observed/ reported. '), (264, 'Project location: production center where project is getting executed or it is location where integration and dispatch of system is planned. Or Customer site in case of delivered product. '), (265, 'Is the NCR is raised on cleared item: this is to record whether previous stage of inspection as per QAP/QCP is cleared by L&T and or customer agency.  '), (266, 'Is the NCR in vendor scope of work: This field is to identify involvement of vendor to carry out RCA. In case of NC is associated with vendor it is required to assign vendor scope “Yes”. Once it is selected as “Yes”, NCR will flow from PLM system to “Sahachar” portal. Vendor will be able to see this NCR through his login in “Sahachar” portal. '), (267, ''), (268, 'NCR owner can review, edit the NCR and promote for Design review. NCR ownership now transferred to responsible senior design engineer'), (269, ''), (270, ''), (271, ''), (272, 'B. Design Review: (Senior Design Engineer)'), (273, 'Designer to review the NCR in PLM and check the content filled by creator. If he found that information is insufficient or incorrect, in that case Designer to demote the NCR with comments. '), (274, 'Designer plays vital role in NCR cycle. He has to decide, '), (275, 'RCA required yes or no '), (276, 'Designer to evaluate the complexity and impact of the NC to decide whether conducting a RCA is necessary for deeper understating and long term prevention of similar issue. '), (277, 'Based on the assessment, make an informed decision on RCA requirement, taking into account the potential risks, recurrence possibilities and impact on product quality and process efficiency. '), (278, 'Note: based on RCA selected Yes/NO system will allow to select appropriate disposition as shown in below table   '), (279, ''), (280, ''), (281, ''), (282, 'Disposition on RCA'), (283, 'Designer to determine disposition such as “accept as it is” in case of minor NC, which doesn’t affect the functioning and fitment. Or “accept partial Quantity” in case of minor deviation from multiple items. Or “confirm FFFI” to be used for deviation which cannot decided unless item is integrated on system. Such deviation can be accepted based on satisfactory fitment of item or functional performance of product. In this case it is responsibility of “Responsible Department” to check Form, Fit, Function & integration of item and update NCR accordingly in “responsible Department 2” stage of PLM cycle.  In case of repair to be carried out “correction/rework” to be selected by designer. If deviation is either not acceptable or beyond repair, under such condition designer can proceed with “Rejecting” the item by selecting “Reject”. '), (284, ''), (285, 'C. Responsible Department: (Responsible Department Manager)'), (286, 'The Responsible Department plays a crucial role in the Root Cause Analysis (RCA) process, ensuring the identification of underlying issues and the implementation of effective corrective actions. Specifically, the responsibilities of the Responsible Department include:'), (287, ''), (288, 'Vendor Involvement in RCA: '), (289, 'Refer to point no. 12 in “create stage”, where NCR creator selects vendor scope of work “Yes”. In that case NCR from PLM will flow down to Sahachar Portal, where vendor can view the NCR.'), (290, 'NCR View in Sahachar Portal: login to Sahachar portal, go to Non-Financial Modules NCR'), (291, ''), (292, '2. Action by Supplier: '), (293, 'When NCR is in “Responsible Department” stage, vendor can provide his inputs against NCR by clicking on button given in action column. '), (294, 'Supplier to provide his input on below points,'), (295, 'Rectification / Correction Actions (Also indicate Containment actions if required)'), (296, 'Root Causes (Why-Why Analysis /\\xa0Fish-bone diagram)'), (297, 'Corrective actions to prevent re-occurrence\\xa0'), (298, 'Corrective Action by'), (299, 'Root Cause Analysis Document'), (300, ''), (301, ''), (302, 'Responsible Department scope in RCA:'), (303, ''), (304, 'Conducting Root Cause Analysis (RCA): Utilize the 5-Why technique to systematically investigate the underlying causes of the reported non-conformity, aiming to identify the fundamental factors contributing to the issue.'), (305, 'Entering 5-Why Analysis: Record each step of the 5-Why analysis accurately within the designated field in the Product Lifecycle Management (PLM) system, providing a comprehensive understanding of the causal chain leading to the non-conformity.'), (306, 'Assessing Correction Requirement: Review the correction suggestion proposed by the designer and, if necessary, decide whether corrections are required, evaluating the impact on the product quality and manufacturing process.'), (307, 'Determining Corrective Actions: Choose at least one suitable corrective action based on the RCA findings, focusing on addressing the root causes and preventing the recurrence of similar non-conformities in the future.'), (308, 'Assigning Responsible Personnel: Identify and assign individuals responsible for executing the corrective actions, considering their expertise and involvement in the specific aspects related to the identified non-conformity. Select up to four individuals as necessary, each associated with a distinct corrective action.'), (309, 'Selecting Corrective Action Category: Utilize the provided drop-down menu to categorize the corrective actions, ensuring accurate classification based on the nature of the identified corrective measures.'), (310, 'Specifying Corrective Action Details: Provide a clear and comprehensive description of the corrective actions to be undertaken, outlining the steps and strategies that will be implemented to address the root causes and improve the overall quality of the manufacturing process.'), (311, ''), (312, ''), (313, 'Route: Applicable for all cycles except cycle 1 (Design Confirmation) '), (314, 'Identify assignees for CA: Once RCA & CA details are filled, owner need to create the auto route for RCA, CA Confirmation. First step is to add all CA responsible person identified during RCA stage, along with CA person he may add additional team members as per requirement. This may be from welding/painting or any other team for RCA Confirmation. '), (315, ''), (316, ''), (317, ''), (318, ''), (319, ''), (320, ''), (321, ''), (322, ''), (323, 'Create the Route:'), (324, ''), (325, ''), (326, ''), (327, 'Confirmation from Route assignees: Route assigned to individual team members should review the task assigned to them and accept the task in route. '), (328, ''), (329, 'Design Review 2 (Senior Design Engineer)'), (330, 'Below is the role of designer for review of RCA,'), (331, 'Assessment of Root Cause Analysis (RCA): Thoroughly review the RCA report submitted by the Responsible Department, evaluating the effectiveness and comprehensiveness of the analysis in identifying the underlying causes of the non-conformity.'), (332, 'Acceptance or Rejection of RCA: Make an informed decision to either accept or reject the RCA, considering the validity of the findings and the adequacy of the proposed corrective actions. Provide a clear rationale in case of non-acceptance, outlining the specific shortcomings or deficiencies identified in the analysis.'), (333, 'Referral to Customer: Determine whether the non-conformity needs to be referred to the customer for further review or action. Assess the potential impact on the customer and the product/service, and make a decision accordingly. If necessary, add any relevant comments provided by the customer.'), (334, 'Technical Justification for Corrective action Acceptance: Provide a technical justification for accepting the Corrective Action Plan (CAP) proposed by the Responsible Department. Highlight the alignment of the proposed actions with the identified root causes and their effectiveness in preventing the recurrence of similar non-conformities.'), (335, 'Modification of Disposition: Modify the disposition provided in the earlier stages if deemed necessary, based on the updated information and the findings of the RCA. Ensure that the modified disposition accurately reflects the current understanding of the non-conformity and the actions taken to address it.'), (336, ''), (337, ''), (338, 'Verify RCA & CA: (QC Manager)'), (339, 'Verification of RCA and CA: Thoroughly review the submitted Root Cause Analysis (RCA) and the proposed Corrective Actions to assess their validity and effectiveness in addressing the identified non-conformity.'), (340, 'Acceptance or Rejection of Corrective Action Plan: Make a well-informed decision to either accept or reject the proposed Corrective Action Plan (CAP), considering their ability to effectively resolve the identified root causes. Provide a clear rationale in case of non-acceptance, detailing any deficiencies or inadequacies observed in the proposed actions.'), (341, 'Release Decision for Next Stage: Determine whether the deviation component is suitable for release to the next stage of the manufacturing process, taking into account the effectiveness of the proposed Corrective Action Plan (CAP) in mitigating the identified non-conformity. Ensure that the decision aligns with the established quality standards and regulatory requirements.'), (342, ''), (343, 'Responsible Department 2: (Responsible Department Manager)'), (344, 'In case of cycle 3 i.e. Confirm FFFI where no correction is involved, responsible department to get the part checked during Integration. Update the fields in PLM and promote NCR to next stage.'), (345, ''), (346, ''), (347, 'Implement Correction: (Responsible Department Manager)'), (348, 'In case of cycle 4 i.e. Correction & RCA which involves verification or correction in part, responsible department is to take below action.'), (349, 'Implementation of Proposed Correction: Execute proposed correction measures as outlined in the Non-Conformity Report (NCR) cycle, ensuring that necessary actions are taken to address identified root causes & resolve non-conformity effectively. '), (350, 'Confirmation of Part Compliance: Conduct thorough verification to confirm whether the part now meets the required specifications and quality standards, ensuring that all the necessary corrections have been successfully implemented and that the product now complies with the defined requirements.'), (351, 'Input in PLM System: Enter the necessary details and outcomes of the correction process into the Product Lifecycle Management (PLM) system, providing accurate and comprehensive information regarding the completed corrections, including any additional notes or observations that may be relevant for future reference.'), (352, ''), (353, 'Verify Correction: (QC Engineer)'), (354, 'Physical Inspection of the Item: Conduct a meticulous physical inspection of the item to verify whether the implemented correction has effectively rectified the identified defect, ensuring that the product now meets the acceptable quality standards and specifications.'), (355, 'Decision on Defect Rectification:  Determine whether the defect has been rectified to an acceptable limit, considering the predefined quality thresholds and specifications. Provide a clear and well-founded decision on whether the implemented correction aligns with the required quality standards.'), (356, 'Comprehensive Rectification Check: Ensure that the rectification process has been completed comprehensively in all respects, addressing not only the identified defect but also any potential underlying issues or related concerns to prevent the recurrence of similar non-conformities.'), (357, 'Addition of Correction Implementation Comments: Provide detailed and insightful comments regarding the implementation of the correction, including any observations, findings, or recommendations for further improvement or quality enhancement. Ensure that the comments accurately reflect the outcomes of the verification process and serve as valuable insights for future reference.'), (358, 'Decision on Quality Review Meeting: Determine whether the outcomes of the verification process warrant discussion with the top management in the upcoming quality review meeting. Assess the significance of the findings and the potential impact on the overall quality management system to make an informed decision'), (359, ''), (360, ''), (361, ''), (362, 'Implement CA: (Route Owner) '), (363, 'Action Implementation: Execute the specific corrective actions assigned based on the findings of the Root Cause Analysis (RCA), ensuring that the outlined steps are carried out effectively and in line with the established guidelines and best practices.'), (364, ''), (365, 'Verify CA: (QC Manager)'), (366, 'Comprehensive Review: Conduct a comprehensive review of the implemented corrective actions, taking into account the details outlined in the Root Cause Analysis (RCA) report and ensuring that the actions taken align with the identified root causes and the recommended solutions.'), (367, 'Alignment Check: Verify whether the corrective actions being undertaken are in line with the actions identified during the RCA process, evaluating their effectiveness in addressing the root causes and preventing the recurrence of similar non-conformities in the future.'), (368, 'Feedback and Guidance: Provide necessary feedback and guidance to the team members involved in the implementation process, ensuring that any deviations from the RCA recommendations are addressed promptly and that the corrective actions are adjusted accordingly to ensure their alignment with the RCA findings. In case it is found that implemented CA is not in line with RCA, QC person can demote the NCR to prevision stage. '), (369, ''), (370, ''), (371, ''), (372, ' General Timeline for Various Stages of NC Handling'), (373, ''), (374, ' Utilizing NC Data in Management Review'), (375, ' Data Collection and Analysis'), (376, 'Critical NC’s that had a major impact on the project cycle or actions of NC’s that can be deployed horizontally shall be presented in the Management Review meetings.'), (377, 'Analysis of these NC’s shall be done through the dashboards and against the defined KPI’s.'), (378, ''), (379, ' Key Performance Indicators (KPIs) for NC Handling'), (380, 'Repeated NC’s (No. of items having more than 1 NC / Total No. of items having NC) '), (381, 'NCR Trend Year-wise'), (382, \"No. of NC's closed on-time \"), (383, 'The above can be reported and presented in the Management Review Meetings / suitable forums.'), (384, ''), (385, ''), (386, ''), (387, ''), (388, ''), (389, ''), (390, ''), (391, 'Roles & Responsibilities'), (392, ''), (393, 'References (Applicable Documents)'), (394, '        --'), (395, 'Abbreviations: '), (396, 'Corrective Action (CA)'), (397, 'Corrective Action Plan (CAP)'), (398, 'Form Fit Function (FFF)'), (399, 'Non Conformity Report (NCR) '), (400, 'Product Lifecycle Management (PLM)'), (401, 'Quality Control (QC) '), (402, 'Root Cause Analysis (RCA) '), (403, ''), (404, 'Additional Points'), (405, 'FAQ'), (406, 'What is the purpose of a Root Cause Analysis (RCA)?'), (407, 'RCA is conducted to identify the underlying reasons behind non-conformities, aiming to prevent their recurrence and improve the overall quality and efficiency of the manufacturing process.'), (408, ''), (409, 'Why is the involvement of multiple departments crucial in the RCA process?'), (410, 'Involving multiple departments ensures a comprehensive understanding of the non-conformity and facilitates the implementation of effective corrective actions that address various aspects of the issue.'), (411, ''), (412, 'What is the role of the Responsible Department in the corrective action process?'), (413, 'The Responsible Department is responsible for executing the corrective actions identified during the RCA, ensuring that the necessary steps are taken to rectify the identified non-conformities.'), (414, ''), (415, 'How does the Quality Control Manager contribute to the process?'), (416, 'The Quality Control Manager verifies the implemented corrections, ensuring that they meet the required specifications and standards. They also oversee the decision-making process regarding the need for further quality review meetings.'), (417, ''), (418, 'Why is continuous documentation crucial throughout the process?'), (419, 'Documentation facilitates transparency, accountability, and the tracking of progress at each stage of the process. It also serves as a valuable reference for future analysis and improvement initiatives.'), (420, ''), (421, 'How do team members ensure the successful implementation of corrective actions?'), (422, 'Team members must execute the corrective actions as outlined, monitor their progress, and promptly report any deviations or challenges. They should ensure that the actions align with the RCA recommendations and effectively address the identified root causes.'), (423, ''), (424, \"What is the significance of the Quality Manager's review in the corrective action process?\"), (425, \"The Quality Manager's review ensures that the corrective actions align with the RCA findings, promoting consistency and effectiveness in the resolution of non-conformities and contributing to the enhancement of overall product quality and process efficiency.\"), (426, ''), (427, ' Revision History'), (428, '')], 'text extraction done')\n"
     ]
    }
   ],
   "source": [
    "from docx import Document as DocxDocument\n",
    "\n",
    "file_path=\"/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\"\n",
    "\n",
    "def process_docx(file_path):\n",
    "    \"\"\" Process .docx file and extract text \"\"\"\n",
    "    try:\n",
    "        doc = DocxDocument(file_path)\n",
    "        text_by_paragraph = [(i + 1, para.text) for i, para in enumerate(doc.paragraphs)]\n",
    "        return text_by_paragraph, \"text extraction done\" \n",
    "    except Exception as e:\n",
    "        return [], f\"DOCX FILE ERROR : {e}\"\n",
    "    \n",
    "\n",
    "    \n",
    "print(process_docx(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script starting...\n",
      "Starting document analysis for: /home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\n",
      "Processing file: /home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\n",
      "Conversion complete. Text length: 43765\n",
      "Found 461 paragraphs\n",
      "Processed 461 text segments across 1 pages\n",
      "Found 0 explicit page breaks\n",
      "No page breaks found but document is long. Estimating pages...\n",
      "Estimated 23 pages based on paragraph count\n",
      "Total extracted characters: 42561\n",
      "Creating chunks from 461 segments...\n",
      "Created 78 chunks\n",
      "Writing output to: /home/aicoe/Desktop/document_analysis/chunks_with_pages_600_50.txt\n",
      "Analysis complete. File saved: /home/aicoe/Desktop/document_analysis/chunks_with_pages_600_50.txt\n",
      "Input file had 42561 characters, output has 46330 characters\n",
      "Script completed.\n"
     ]
    }
   ],
   "source": [
    "import pypandoc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def process_docx_with_pages(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from docx file while preserving page number information using pypandoc.\n",
    "    Returns a list of tuples containing (text, page_number) for each paragraph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        markdown_text = pypandoc.convert_file(\n",
    "            file_path,\n",
    "            'markdown',\n",
    "            extra_args=['--extract-media=.', '--wrap=none']\n",
    "        )\n",
    "        \n",
    "        print(f\"Conversion complete. Text length: {len(markdown_text)}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "      \n",
    "        paragraphs = [p for p in markdown_text.split('\\n') if p.strip()]\n",
    "        print(f\"Found {len(paragraphs)} paragraphs\")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "        text_with_pages = []\n",
    "        current_page = 1\n",
    "        page_breaks_found = 0\n",
    "        \n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if '\\f' in para or '\\\\pagebreak' in para or '\\\\newpage' in para:\n",
    "                current_page += 1\n",
    "                page_breaks_found += 1\n",
    "                continue\n",
    "            \n",
    "            if para.strip():\n",
    "                text_with_pages.append((para.strip(), current_page))\n",
    "        \n",
    "        print(f\"Processed {len(text_with_pages)} text segments across {current_page} pages\")\n",
    "        print(f\"Found {page_breaks_found} explicit page breaks\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    \n",
    "        if current_page == 1 and len(text_with_pages) > 30:\n",
    "            print(\"No page breaks found but document is long. Estimating pages...\")\n",
    "    \n",
    "            estimated_pages = max(1, len(text_with_pages) // 20)\n",
    "            paragraphs_per_page = len(text_with_pages) / estimated_pages\n",
    "            \n",
    "          \n",
    "            new_text_with_pages = []\n",
    "            for i, (text, _) in enumerate(text_with_pages):\n",
    "                estimated_page = min(estimated_pages, 1 + i // int(paragraphs_per_page))\n",
    "                new_text_with_pages.append((text, estimated_page))\n",
    "            \n",
    "            text_with_pages = new_text_with_pages\n",
    "            current_page = estimated_pages\n",
    "            print(f\"Estimated {current_page} pages based on paragraph count\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        return text_with_pages, f\"text extraction with page numbers done, found {current_page} pages\"\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback_str = traceback.format_exc()\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(traceback_str)\n",
    "        sys.stdout.flush()\n",
    "        return [], f\"ERROR: {e}\"\n",
    "\n",
    "def create_chunks_with_pages(text_with_pages, chunk_size=600, overlap_size=50):\n",
    "    \"\"\"\n",
    "    Create chunks using sliding window approach while tracking page numbers.\n",
    "    Returns list of tuples containing (chunk_text, start_page, end_page).\n",
    "    \"\"\"\n",
    "    print(f\"Creating chunks from {len(text_with_pages)} segments...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    chunks = []\n",
    "    current_text = \"\"\n",
    "    current_pages = set()\n",
    "    \n",
    "    for text, page in text_with_pages:\n",
    "        current_text += text + \" \"\n",
    "        current_pages.add(page)\n",
    "        \n",
    "        while len(current_text) >= chunk_size:\n",
    "            chunk = current_text[:chunk_size].strip()\n",
    "            if chunk:\n",
    "                chunks.append((chunk, min(current_pages), max(current_pages)))\n",
    "            \n",
    "            current_text = current_text[chunk_size-overlap_size:].strip()\n",
    "            if not current_text:\n",
    "                current_pages = set()\n",
    "    \n",
    "    if current_text.strip():\n",
    "        chunks.append((current_text.strip(), min(current_pages), max(current_pages)))\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    sys.stdout.flush()\n",
    "    return chunks\n",
    "\n",
    "def analyze_and_save_document_processing(file_path, output_dir=\"/home/aicoe/Desktop/document_analysis/\"):\n",
    "    print(f\"Starting document analysis for: {file_path}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    text_with_pages, status = process_docx_with_pages(file_path)\n",
    "    if not text_with_pages:\n",
    "        print(f\"Document processing failed: {status}\")\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "    \n",
    "    total_chars = sum(len(text) for text, _ in text_with_pages)\n",
    "    print(f\"Total extracted characters: {total_chars}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if total_chars <600:\n",
    "        print(f\"Document too small (only {total_chars} characters). Skipping processing.\")\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "    \n",
    "    chunk_size = 600\n",
    "    overlap = 50\n",
    "    chunks = create_chunks_with_pages(text_with_pages, chunk_size, overlap)\n",
    "    \n",
    "    chunk_file_path = os.path.join(output_dir, f\"chunks_with_pages_{chunk_size}_{overlap}.txt\")\n",
    "    print(f\"Writing output to: {chunk_file_path}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"DOCUMENT CHUNKS (size={chunk_size}, overlap={overlap})\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        total_chunk_chars = 0\n",
    "        \n",
    "        for i, (chunk, start_page, end_page) in enumerate(chunks, 1):\n",
    "            f.write(f\"Chunk {i} ({len(chunk)} chars) - Pages {start_page}-{end_page}:\\n\")\n",
    "            f.write(chunk + \"\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            total_chunk_chars += len(chunk)\n",
    "        \n",
    "        f.write(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "        f.write(f\"\\nTotal characters in chunks: {total_chunk_chars}\")\n",
    "    \n",
    "    print(f\"Analysis complete. File saved: {chunk_file_path}\")\n",
    "    print(f\"Input file had {total_chars} characters, output has {total_chunk_chars} characters\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script starting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    file_path = \"/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\"\n",
    "    analyze_and_save_document_processing(file_path)\n",
    "    \n",
    "    print(\"Script completed.\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script starting...\n",
      "Starting document analysis for: /home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\n",
      "Processing file: /home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\n",
      "Conversion complete. Text length: 43765\n",
      "Found 369 paragraphs\n",
      "Processed 369 text segments across 1 pages\n",
      "Found 0 explicit page breaks\n",
      "No page breaks found but document is long. Estimating pages...\n",
      "Estimated 18 pages based on paragraph count\n",
      "Total extracted characters: 32427\n",
      "Creating chunks from 369 segments...\n",
      "Created 60 chunks\n",
      "Writing output to: /home/aicoe/Desktop/document_analysis/chunks_with_pages_600_50.txt\n",
      "Analysis complete. File saved: /home/aicoe/Desktop/document_analysis/chunks_with_pages_600_50.txt\n",
      "Input file had 32427 characters, output has 35627 characters\n",
      "Script completed.\n"
     ]
    }
   ],
   "source": [
    "import pypandoc\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def process_docx_with_pages(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from docx file while preserving page number information using pypandoc.\n",
    "    Applies regex-based filtering to clean extracted text.\n",
    "    Returns a list of tuples containing (text, page_number) for each paragraph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        markdown_text = pypandoc.convert_file(\n",
    "            file_path,\n",
    "            'markdown',\n",
    "            extra_args=['--extract-media=.', '--wrap=none']\n",
    "        )\n",
    "        \n",
    "        print(f\"Conversion complete. Text length: {len(markdown_text)}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "        filtered_text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', markdown_text) \n",
    "        filtered_text = re.sub(r'\\{width=\"[^\"]*\"[^}]*\\}', '', filtered_text)  \n",
    "        filtered_text = re.sub(r'[+-]{3,}', '', filtered_text)  \n",
    "        filtered_text = re.sub(r'\\|[^|]*\\|', '', filtered_text)  \n",
    "        filtered_text = re.sub(r'^\\+[-=]+\\+$', '', filtered_text, flags=re.MULTILINE)  \n",
    "        filtered_text = re.sub(r'[-=]{3,}', '', filtered_text)  \n",
    "        filtered_text = re.sub(r'[|+]', '', filtered_text)  \n",
    "        #filtered_text = re.sub(r'\\s+', ' ', filtered_text) \n",
    "        \n",
    "        paragraphs = [p for p in filtered_text.split('\\n') if p.strip()]\n",
    "        print(f\"Found {len(paragraphs)} paragraphs\")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "        text_with_pages = []\n",
    "        current_page = 1\n",
    "        page_breaks_found = 0\n",
    "        \n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if '\\f' in para or '\\\\pagebreak' in para or '\\\\newpage' in para:\n",
    "                current_page += 1\n",
    "                page_breaks_found += 1\n",
    "                continue\n",
    "            \n",
    "            if para.strip():\n",
    "                text_with_pages.append((para.strip(), current_page))\n",
    "        \n",
    "        print(f\"Processed {len(text_with_pages)} text segments across {current_page} pages\")\n",
    "        print(f\"Found {page_breaks_found} explicit page breaks\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if current_page == 1 and len(text_with_pages) > 30:\n",
    "            print(\"No page breaks found but document is long. Estimating pages...\")\n",
    "    \n",
    "            estimated_pages = max(1, len(text_with_pages) // 20)\n",
    "            paragraphs_per_page = len(text_with_pages) / estimated_pages\n",
    "            \n",
    "          \n",
    "            new_text_with_pages = []\n",
    "            for i, (text, _) in enumerate(text_with_pages):\n",
    "                estimated_page = min(estimated_pages, 1 + i // int(paragraphs_per_page))\n",
    "                new_text_with_pages.append((text, estimated_page))\n",
    "            \n",
    "            text_with_pages = new_text_with_pages\n",
    "            current_page = estimated_pages\n",
    "            print(f\"Estimated {current_page} pages based on paragraph count\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        return text_with_pages, f\"text extraction with page numbers done, found {current_page} pages\"\n",
    "   \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback_str = traceback.format_exc()\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(traceback_str)\n",
    "        sys.stdout.flush()\n",
    "        return [], f\"ERROR: {e}\"\n",
    "\n",
    "    \n",
    "def create_chunks_with_pages(text_with_pages, chunk_size=600, overlap_size=50):\n",
    "    \"\"\"\n",
    "    Create chunks using sliding window approach while tracking page numbers.\n",
    "    Returns list of tuples containing (chunk_text, start_page, end_page).\n",
    "    \"\"\"\n",
    "    print(f\"Creating chunks from {len(text_with_pages)} segments...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    chunks = []\n",
    "    current_text = \"\"\n",
    "    current_pages = set()\n",
    "    \n",
    "    for text, page in text_with_pages:\n",
    "        current_text += text + \" \"\n",
    "        current_pages.add(page)\n",
    "        \n",
    "        while len(current_text) >= chunk_size:\n",
    "            chunk = current_text[:chunk_size].strip()\n",
    "            if chunk:\n",
    "                chunks.append((chunk, min(current_pages), max(current_pages)))\n",
    "            \n",
    "            current_text = current_text[chunk_size-overlap_size:].strip()\n",
    "            if not current_text:\n",
    "                current_pages = set()\n",
    "    \n",
    "    if current_text.strip():\n",
    "        chunks.append((current_text.strip(), min(current_pages), max(current_pages)))\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    sys.stdout.flush()\n",
    "    return chunks\n",
    "\n",
    "def analyze_and_save_document_processing(file_path, output_dir=\"/home/aicoe/Desktop/document_analysis/\"):\n",
    "    print(f\"Starting document analysis for: {file_path}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    text_with_pages, status = process_docx_with_pages(file_path)\n",
    "    if not text_with_pages:\n",
    "        print(f\"Document processing failed: {status}\")\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "    \n",
    "    total_chars = sum(len(text) for text, _ in text_with_pages)\n",
    "    print(f\"Total extracted characters: {total_chars}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if total_chars <600:\n",
    "        print(f\"Document too small (only {total_chars} characters). Skipping processing.\")\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "    \n",
    "    chunk_size = 600\n",
    "    overlap = 50\n",
    "    chunks = create_chunks_with_pages(text_with_pages, chunk_size, overlap)\n",
    "    \n",
    "    chunk_file_path = os.path.join(output_dir, f\"chunks_with_pages_{chunk_size}_{overlap}.txt\")\n",
    "    print(f\"Writing output to: {chunk_file_path}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"DOCUMENT CHUNKS (size={chunk_size}, overlap={overlap})\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        total_chunk_chars = 0\n",
    "        \n",
    "        for i, (chunk, start_page, end_page) in enumerate(chunks, 1):\n",
    "            f.write(f\"Chunk {i} ({len(chunk)} chars) - Pages {start_page}-{end_page}:\\n\")\n",
    "            f.write(chunk + \"\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            total_chunk_chars += len(chunk)\n",
    "        \n",
    "        f.write(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "        f.write(f\"\\nTotal characters in chunks: {total_chunk_chars}\")\n",
    "    \n",
    "    print(f\"Analysis complete. File saved: {chunk_file_path}\")\n",
    "    print(f\"Input file had {total_chars} characters, output has {total_chunk_chars} characters\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script starting...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    file_path = \"/home/aicoe/Desktop/QA_pdf_not_open/QC_Documents/SOP-Nonconformity handling in PLM R0.docx\"\n",
    "    analyze_and_save_document_processing(file_path)\n",
    "    \n",
    "    print(\"Script completed.\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pymilvus import connections, Collection\n",
    "connections.connect(\"default\", host=\"172.16.33.13\", port= 19530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
